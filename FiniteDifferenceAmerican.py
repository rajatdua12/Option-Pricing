#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Feb 24 22:46:25 2020

@author: rajatdua
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Oct 15 20:59:39 2019

@author: rajatdua
"""

"""
In this problem, I have defined a function fdAmerican, to price American option value 
using the Gauss-Seidel method to solve a Crank-Nicolson formulation of the finite difference 
scheme, with input parameters: callput to determine whether the option is call or put, spot 
price (S0), strike price (K), yield to maturity (r), Time to Maturity (T) (in years), volatility 
(sigma), dividend yield rate (q), M+1 is the number of spatial steps in the uniform grid N+1 is 
the number of time steps in the uniform grid, Smax is the value where the upper spatial boundary 
condition is applied, and tol is the tolerance which is the stopping criteria for the Gauss-Seidel 
iteration method. 

The ratio of dt (=T/N) and dS^2 (=(Smax/M)^2) should be less than 1 in order to ensure that 
the finite difference scheme is stable or else the price output of the function will be too 
high (positive) or too low (negative). The output of this function is a current American option 
value corresponding to the current stock price, and the difference between option value calculated 
from black-scholes formula and Crank-Nicolson formulation of the finite difference scheme as price error.  I have compared the final value with the output generated from bsformula for the option value and found out that the final value from fdAmerican is very close to the output generated by the bsformula. I created a plot comparing the variation of price Error between bsformula and fdAmerican with Number of spatial steps(M),
"""

import sys
from BS import bsformula
import numpy as np
import scipy.linalg as linalg
import matplotlib.pyplot as plt

def fdAmerican(callput, S0, K, r, T, sigma, q, M, N, S_max, tol):
    
    #dS = S_max / float(M)
    dt = T / float(N)
    i_values = np.arange(M+1)
    j_values = np.arange(N+1)
    boundary_conds = np.linspace(0, S_max, M+1)
    
    if callput == 1:
        payoffs = np.maximum(
            boundary_conds[1:M]-K, 0)

        past_values = payoffs
    
        lower_boundary_values = np.maximum(S_max - K, 0) * \
                           np.exp(-r * dt * (N-j_values))
        upper_boundary_values = 0* j_values     
        
    elif callput == -1:
        payoffs = np.maximum(
            K-boundary_conds[1:M], 0)
        
        past_values = payoffs
        
        upper_boundary_values = K * \
                           np.exp(-r * dt * (N-j_values))
        lower_boundary_values = 0* j_values
    
    alpha = 0.25*dt*((sigma**2)*(i_values**2) - r* i_values)
    beta = -dt*0.5*((sigma**2)*(i_values**2) +r)
    gamma = 0.25*dt*((sigma**2)*(i_values**2) +r*i_values)
    M1 = -np.diag(alpha[2:M], -1) + \
                  np.diag(1-beta[1:M]) - \
                  np.diag(gamma[1:M-1], 1)
    M2 = np.diag(alpha[2:M], -1) + \
                  np.diag(1+beta[1:M]) + \
                  np.diag(gamma[1:M-1], 1)
    
    P, L, U = linalg.lu(M1)
    aux = np.zeros(M-1)
    new_values = np.zeros(M-1)

    for j in reversed(range(N)):
        aux[0] = alpha[1]*(upper_boundary_values[j] + upper_boundary_values[j+1])
        aux[M-2]= gamma[M-1]*(lower_boundary_values[j]+ lower_boundary_values[j+1])
        
        rhs = np.dot(M2, past_values) + aux
        old_values = np.copy(past_values)
        error = sys.float_info.max

        while tol < error:
            new_values[0] = \
                max(payoffs[0],
                    old_values[0] +
                    1.0/(1-beta[1]) *
                    (rhs[0] -
                     (1-beta[1])*old_values[0] +
                     (gamma[1]*old_values[1])))

            for k in range(M-2)[1:]:
                new_values[k] = \
                    max(payoffs[k],
                        old_values[k] + 
                        1.0/(1-beta[k+1]) *
                        (rhs[k] +
                         alpha[k+1]*new_values[k-1] -
                         (1-beta[k+1])*old_values[k] +
                         gamma[k+1]*old_values[k+1]))

            new_values[-1] = \
                max(payoffs[-1],
                    old_values[-1] +
                    1.0/(1-beta[-2]) *
                    (rhs[-1] +
                     alpha[-2]*new_values[-2] -
                     (1-beta[-2])*old_values[-1]))

            error = np.linalg.norm(new_values - old_values)
            old_values = np.copy(new_values)

        past_values = np.copy(new_values)

    values = np.concatenate(([upper_boundary_values[0]],
                                  new_values,
                                  [0]))
        
    price = bsformula(callput = callput, S0 = S0, K = K, r = r, T = T, sigma = sigma, q = q)[0]
    diff = price - np.interp(S0, boundary_conds, values)
    return np.interp(S0, boundary_conds, values), diff

if __name__ == "__main__":
    
    print(fdAmerican(1, 50, 50, 0.05, 5./12., 0.25, 0.03, 100, 1000, 100, 0.001))
    
    M1 = np.linspace(60, 600, 10)
    
    Y = [fdAmerican(1, 50, 50, 0.05, 5./12., 0.25, 0.03, int(m), 1000, 100, 0.001)[1] for m in M1]
    
    X = M1
    
    plt.figure(1)
    plt.plot(X, Y, 'r')
    plt.title('Price Error Vs Number of Spatial steps')
    plt.xlabel('Number of Spatial steps')
    plt.ylabel('Price Error')


    